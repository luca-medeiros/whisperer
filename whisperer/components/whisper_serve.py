import time
from typing import List
from concurrent.futures import ThreadPoolExecutor, TimeoutError

import lightning as L
import torch

from whisperer.CONST import INFERENCE_REQUEST_TIMEOUT, KEEP_ALIVE_TIMEOUT
from whisperer.models import WhisperModel
from whisperer.utility.data_io import Data, DataBatch, TimeoutException


class WhisperServe(L.LightningWork):
    """The WhisperServe handles the prediction.

    It initializes a model and expose an API to handle incoming requests and generate predictions.
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._model = None

    def build_model(self):
        """The `build_model(...)` method returns a model and the returned model is set to `self._model` state."""

        print("loading model...")
        self._model = WhisperModel()
        print("model loaded")

    @torch.inference_mode()
    def predict(self, video_urls: List[Data], entry_time: int):
        if time.time() - entry_time > INFERENCE_REQUEST_TIMEOUT:
            raise TimeoutException()

        torch.cuda.empty_cache()
        results = self._model(video_urls)
        return results

    def run(self):
        import subprocess

        import uvicorn
        from fastapi import FastAPI
        from fastapi.middleware.cors import CORSMiddleware

        if torch.cuda.is_available():
            subprocess.run("nvidia-smi", shell=True)

        if self._model is None:
            self.build_model()

        self._fastapi_app = app = FastAPI()
        app.POOL: ThreadPoolExecutor = None

        @app.on_event("startup")
        def startup_event():
            app.POOL = ThreadPoolExecutor(max_workers=1)

        @app.on_event("shutdown")
        def shutdown_event():
            app.POOL.shutdown(wait=False)

        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        @app.get("/api/health")
        def health():
            return True

        @app.post("/api/predict")
        def predict_api(data: DataBatch):
            """Dream a muse. Defines the REST API which takes the text prompt, number of images and image size in the
            request body.

            This API returns an image generated by the model in base64 format.
            """
            try:
                entry_time = time.time()
                print(f"batch size: {len(data.batch)}")
                result = app.POOL.submit(
                    self.predict,
                    data.batch,
                    entry_time=entry_time,
                ).result(timeout=INFERENCE_REQUEST_TIMEOUT)
                return result
            except (TimeoutError, TimeoutException):
                # hack: once there is a timeout then all requests after that is getting timeout
                # old_pool = app.POOL
                # app.POOL = ThreadPoolExecutor(max_workers=1)
                # old_pool.shutdown(wait=False)
                # signal.signal(signal.SIGINT, lambda sig, frame: exit_threads(old_pool))
                raise TimeoutException()

        uvicorn.run(app,
                    host=self.host,
                    port=self.port,
                    timeout_keep_alive=KEEP_ALIVE_TIMEOUT,
                    access_log=False,
                    loop="uvloop")
